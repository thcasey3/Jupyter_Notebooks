{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Measuring the correlation between data content and filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: **Thomas Casey**\n",
    "##### Location: University of California Santa Barbara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Business Understanding](#Business-Understanding)\n",
    "2. [Data Understanding](#Data-Understanding)\n",
    "3. [Data Preparation](#Data-Preparations)\n",
    "4. [Modeling](#Method-Description)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Deployment](#Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Business Understanding\"> </a>\n",
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can undocumented and unstructured data be structured and interpreted with an acceptable level of confidence based only on the raw content of the data and filenames associated with the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data Understanding\"> </a>\n",
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are generated using an experiment called Overhauser Dynamic Nuclear Polarization (ODNP). The data should exist in blocks, meaning certain sizeed collections of files and folders consititute one piece of data. The routine that collects and categorizes the data is known and some aspects of the structure of the data are already understood:\n",
    "1. Each \"sample\" corresponds to a base folder that contains folders named \"1\" through \"33\", \"304\", \"503, \"700\", \"701\", and also two .mat files OR two .csv files; one named \"power\" and the other named \"t1_powers\". The folders can either be of type null or contain additional files that yield a one dimensional spectrum or two dimensional spectra in a proprietary format. The .mat or .csv files contain lists of continuous measurements of microwave power level made over the span of approximately two hours. \n",
    "2. One of the one dimensional spectra is considered an \"off\" spectrum and several others are to be compared to it once it is identified. Most of the spectra are one dimensional and should present a smooth trend in changing intensities if organized properly.\n",
    "3. A subset are two dimensional spectra and should also present a smooth trend in changing intensities. \n",
    "4. The measurements in the .mat and .csv files should correspond in time to the time stamp of certain files in each numbered folder. These arrays should be condensed to a length that matches the total number of spectra by sectioning based on time intervals and averaging each section individually. \n",
    "5. The one dimensional spectra must be processed and condensed to a single amplitude that is the integral of the spectrum. The two dimensional spectra must be condensed to a single number \"T1\" that is the result of processing, integrating, and fitting the trend in integrals within the 2D set to an exponential function.\n",
    "6. When properly arranged, the one dimensional data should yield a curve of spectral amplitudes that increases asmyptotically and the two dimensional data another curve of \"T1\" values that increases linearly. The theory behind and form of the analytical models for fitting the data can be found in [Prog. Nuc. Mag. Res.](http://dx.doi.org/10.1016/j.pnmrs.2013.06.001) and [Methods in Enzymology](https://doi.org/10.1016/bs.mie.2018.09.024).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data Preparation\"> </a>\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For handling this data most efficiently I use a python package that I helped develop, and currently maintain, called [DNPLab](DNPLab.net). This package contains functions for loading proprietary data formats, processing data, and modeling the data using analytical functions. First it is helpful to take apart one sample folder to learn how to handle the rest of the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dnplab                      # import the DNPLab package\n",
    "from dnplab.dnpImport import load  # condense the syntax for loading data\n",
    "import os                          # import os for using path tools\n",
    "import numpy as np                 # import numpy for useful tools\n",
    "import matplotlib.pyplot as plt    # use matplotlib to create some plots for visualizing the data\n",
    "\n",
    "base_path = \"../test_set\"\n",
    "paths = os.listdir(base_path)      # create a list of paths to the folders inside the test folder. Each\n",
    "                                   # should refer to one spectrum if it is actual data\n",
    "flag = []\n",
    "for indx, path in enumerate(       # loop through paths attempting to interpret them as data\n",
    "    paths\n",
    "):  \n",
    "    try:\n",
    "        data = load(os.path.join(base_path, path))\n",
    "        flag.append(\"DATA\")        # successful loading is marked as DATA\n",
    "    except:\n",
    "        flag.append(\"NULL\")        # errors marked as NULL\n",
    "        continue\n",
    "\n",
    "truth_table = np.column_stack(     # construct a 2D list where each folder path is DATA or NULL\n",
    "    (paths, flag)\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing some of the data shows one dimensional spectra are in the form of free-induction decays,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(os.path.join(base_path, paths[10]))\n",
    "plt.plot(data.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two dimensional data are as well,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(os.path.join(base_path, paths[1]))\n",
    "plt.plot(data.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the relevant data are identified, it must be arrange for modeling. I start by condensing to just the usable data from the collection of good and NULL data. I'll store each spectrum to a dictionary where the folder numbers are the keys,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for indx, path in enumerate(paths):\n",
    "    print(indx)\n",
    "    if truth_table[indx, 1] == \"DATA\":\n",
    "        data_dict[truth_table[indx, 0]] = load(os.path.join(base_path, path))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, extract and isolate the target characterisitcs of the data using built in DNPLab functions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_target_1D = []\n",
    "folders_1D = []\n",
    "final_target_2D = []\n",
    "folders_2D = []\n",
    "data_dimensions = []\n",
    "for indx, spec in enumerate(data_dict.keys()):\n",
    "    workspace = dnplab.create_workspace(\"proc\", data_dict[spec])\n",
    "    dnplab.dnpNMR.remove_offset(workspace)\n",
    "    dnplab.dnpNMR.window(workspace, linewidth=10)\n",
    "    dnplab.dnpNMR.fourier_transform(workspace, zero_fill_factor=2)\n",
    "    dnplab.dnpNMR.autophase(workspace, force_positive=False)\n",
    "    data_dimensions.append(workspace[\"proc\"].ndim)\n",
    "    dnplab.dnpTools.integrate(workspace)\n",
    "    if data_dimensions[indx] == 1:\n",
    "        final_target_1D.append(workspace[\"proc\"].values)\n",
    "        folders_1D.append(spec)\n",
    "    elif data_dimensions[indx] == 2:\n",
    "        dnplab.dnpFit.exponential_fit(workspace, type=\"T1\")\n",
    "        final_target_2D.append(workspace[\"fit\"].attrs[\"T1\"])\n",
    "        folders_2D.append(spec)\n",
    "        \n",
    "\n",
    "folders_1D = list(map(int, folders_1D))\n",
    "folders_1D.sort()\n",
    "folders_2D = list(map(str, folders_2D))\n",
    "folders_2D.sort()\n",
    "\n",
    "powers_1D = dnplab.dnpIO.cnsi.get_powers(base_path, \"power\", folders_1D)\n",
    "powers_2D = dnplab.dnpIO.cnsi.get_powers(base_path, \"t1_powers\", folders_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now arranged into the target format; a list of amplitudes of the one dimensional spectra and another list of \"T1\" values for the two dimensional data. Each list has a corresponding power list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Modeling\"> </a>\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models exist for fitting data of this nature and have been programmed into a module called \"dnpHydration\" within the DNPLab package. The data, along with some known parameters in the form of a dictionary, are passed to the function and physical constants are calculated using optimization routines,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydration = {\n",
    "    \"E\": np.array(final_target_1D),\n",
    "    \"E_power\": np.array(powers_1D),\n",
    "    \"T1\": np.array(final_target_2D),\n",
    "    \"T1_power\": np.array(powers_2D),\n",
    "}\n",
    "hydration.update(\n",
    "    {\n",
    "        \"T10\": T10,\n",
    "        \"T100\": T100,\n",
    "        \"spin_C\": spin_C,\n",
    "        \"field\": field,\n",
    "        \"smax_model\": smax_model,\n",
    "        \"t1_interp_method\": t1_interp_method,\n",
    "    }\n",
    ")\n",
    "hyd = dnplab.create_workspace()\n",
    "hyd.add(\"hydration_inputs\", hydration)\n",
    "\n",
    "results = dnplab.dnpHydration.hydration(hyd)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Evaluation\"> </a>\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small set of known data that were previously manually analyzed and published was passed through this routine to confirm the model produces the correct published result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Deployment\"> </a>\n",
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example application was the batch processing of a large dataset for which each of the base folders had clues in their name but there was no available description of the dataset. I processed the entire batch using the procedure above and tried to correlate the clues in the folder names with the results. First I placed the above code into a function to more easily perform the above procedure on an entire batch at once, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(base_path):\n",
    " \n",
    "    # insert above code\n",
    "\n",
    "    return results[\"hydration_results\"][\"k_sigma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I loop over the entire set of base folders creating a dictionary that can be arranged into a simple table,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_list = os.listdir(set_path)\n",
    "\n",
    "descriptive = []\n",
    "for indx, path in enumerate(base_list):\n",
    "    descriptive.append(process(path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is useful to take apart the folder names and give the pieces their own column in the table for more easily making correlations with the descriptive parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir(root_path)  # now get the names of all base folders in the root folder\n",
    "\n",
    "date = []\n",
    "sample = []\n",
    "time = []\n",
    "index = []\n",
    "for indx, name in enumerate(folder_list):\n",
    "    nm = name.split(\"_\")\n",
    "    if \"samp6\" in nm[2]:\n",
    "        date.append(nm[0])\n",
    "        sample.append(nm[2])\n",
    "        time.append(nm[3])\n",
    "        index.append(nm[4])\n",
    "\n",
    "\n",
    "descriptive_dict = {\"date\": date,\n",
    "                 \"sample\": sample,\n",
    "                 \"time\": time,\n",
    "                 \"index\": index,\n",
    "                 \"descriptor\": descriptive,\n",
    "                 }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "sample = []\n",
    "time = []\n",
    "index = []\n",
    "for indx, name in enumerate(folder_list[folder_list.columns[0]]):\n",
    "    nm = name.split(\"_\")\n",
    "    if \"samp6\" in nm[2]:\n",
    "        date.append(nm[0])\n",
    "        sample.append(nm[2])\n",
    "        time.append(nm[3])\n",
    "        index.append(nm[4])\n",
    "\n",
    "\n",
    "descriptive_dict = {\"date\": date,\n",
    "                 \"sample\": sample,\n",
    "                 \"time\": time,\n",
    "                 \"index\": index,\n",
    "                 \"descriptor\": descriptive,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now package the data and create a database to store and interact with the data in table format using SQL. First, initial the database, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use IBM cloud to host the database\n",
    "import ibm_db\n",
    "\n",
    "dsn_hostname = \"\"\n",
    "dsn_uid = \"\"      \n",
    "dsn_pwd = \"\"      \n",
    "\n",
    "dsn_driver = \"{IBM DB2 ODBC DRIVER}\"\n",
    "dsn_database = \"BLUDB\"       \n",
    "dsn_port = \"50000\"     \n",
    "dsn_protocol = \"TCPIP\"  \n",
    "\n",
    "dsn = (\n",
    "    \"DRIVER={0};\"\n",
    "    \"DATABASE={1};\"\n",
    "    \"HOSTNAME={2};\"\n",
    "    \"PORT={3};\"\n",
    "    \"PROTOCOL={4};\"\n",
    "    \"UID={5};\"\n",
    "    \"PWD={6};\").format(dsn_driver, dsn_database, dsn_hostname, dsn_port, dsn_protocol, dsn_uid, dsn_pwd)\n",
    "\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "print (\"Connected to database: \", dsn_database, \"as user: \", dsn_uid, \"on host: \", dsn_hostname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a table,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b977ab33a7de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"descriptor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "make_table = \"CREATE TABLE ODNP(ID INTEGER PRIMARY KEY NOT NULL, DATE VARCHAR(20), SAMPLE CHAR(5), TIME CHAR(2), INDEX CHAR(2), DESCRIPT VARCHAR(20))\"\n",
    "\n",
    "initiate_table = ibm_db.exec_immediate(conn, make_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages pandas and seaborn are useful for handling and visualizing the data. So I use these packages to upload the data to the database and also retireve and analyze,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2e23208c0eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescriptive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"descriptor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "db_eng = create_engine('sqlite://', echo=False)\n",
    "\n",
    "db_frame = pd.dataframe(descriptive_dict) \n",
    "\n",
    "db_frame.to_sql('ODNP', con=db_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now retrieve the data and make some plots plot,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_db_dbi\n",
    "\n",
    "connect = ibm_db_dbi.Connection(conn)\n",
    "\n",
    "sel_stat = \"select * from INSTRUCTOR\"\n",
    "\n",
    "q_dataframe = pandas.read_sql(sel_stat, connect)\n",
    "\n",
    "sample_column = q_dataframe.SAMPLE\n",
    "descriptive_column = q_dataframe.DESCRIPT\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
