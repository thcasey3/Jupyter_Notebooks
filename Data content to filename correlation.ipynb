{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the correlation between data content and filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: **Thomas Casey**\n",
    "##### Location: University of California Santa Barbara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Business Understanding](#Business-Understanding)\n",
    "2. [Data Understanding](#Data-Understanding)\n",
    "3. [Data Preparation](#Data-Preparations)\n",
    "4. [Modeling](#Method-Description)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Deployment](#Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Business Understanding\"> </a>\n",
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can undocumented and unstructured data be structured and interpreted with an acceptable level of confidence based only on the raw content of the data and filenames associated with the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data Understanding\"> </a>\n",
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are generated using an experiment called Overhauser Dynamic Nuclear Polarization (ODNP). The data should exist in blocks, meaning each individual file or folder is only useful as a part of a collection. The routine that collects and organizes the data is known, meaning some aspects of the structure of the data are understood:\n",
    "1. Each \"sample\" corresponds to a base folder. Each folder contains folders named \"1\" through \"33\", \"304\", \"503, \"700\", \"701\", and two .mat files OR two .csv files; one named \"power\" and the other named \"t1_powers\". The folders can be of type null, contain a one dimensional spectrum, or contain two dimensional spectra. The .mat or .csv files contain lists of continuous microwave power measurements made over the span of approximately two hours. \n",
    "2. One of the one dimensional spectra is considered an \"off\" spectrum and several others are compared to it. Most of the spectra are one dimensional and should present changing intensities.\n",
    "3. A subset are two dimensional spectra and should also present differing intensities. \n",
    "4. The measurements in the .mat and .csv files should correspond in time to the collection of each spectrum and should be condensed to a length that matches the total number of spectra by sectioning and averaging each section. \n",
    "5. The one dimensional spectra must be processed and condensed to a single amplitude that is the integral of the spectrum. The two dimensional spectra must be condensed to a single number \"T1\" that is the result of processing, integrating, and fitting the trend in integrals to an exponential function.\n",
    "6. When properly arranged, the data should yield a curve of spectral amplitudes that increases asmyptotically and another curve of \"T1\" values that increases linearly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data Preparation\"> </a>\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For handling this data most efficiently I will use a python package that I helped develop and currently maintain called DNPLab. This package contains functions for loading proprietary data formats, processing data, and modeling the data using analytical functions. First lets isolate one sample folder to learn how to handle the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dnplab                      # import the DNPLab package\n",
    "from dnplab.dnpImport import load  # condense the syntax for loading data\n",
    "import os                          # import os for using path tools\n",
    "import numpy as np                 # import numpy for useful tools\n",
    "import matplotlib.pyplot as plt    # use matplotlib to create some plots for visualizing the data\n",
    "\n",
    "base_path = \"../test_set\"\n",
    "paths = os.listdir(base_path)      # create a list of paths to try\n",
    "\n",
    "flag = []\n",
    "for indx, path in enumerate(       # loop through paths attempting to interpret them as data\n",
    "    paths\n",
    "):  \n",
    "    try:\n",
    "        data = load(os.path.join(base_path, path))\n",
    "        flag.append(\"DATA\")        # successful loading is marked as DATA\n",
    "    except:\n",
    "        flag.append(\"NULL\")        # errors marked as NULL\n",
    "        continue\n",
    "\n",
    "truth_table = np.column_stack(     # construct a 2D list to differentiate DATA from NULL folders\n",
    "    (paths, flag)\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize some of the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(os.path.join(base_path, paths[10]))\n",
    "plt.plot(data.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like one dimensional data, lets look at a two dimensional set,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(os.path.join(base_path, paths[1]))\n",
    "plt.plot(data.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the relevant data identified, lets arrange it for modeling. Start by condensing to just the usable data from the collection of good and NULL data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for indx, path in enumerate(paths):\n",
    "    print(indx)\n",
    "    if truth_table[indx, 1] == \"DATA\":\n",
    "        data_dict[truth_table[indx, 0]] = load(os.path.join(base_path, path))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets extract the isolate the target characterisitcs of the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_target_1D = []\n",
    "folders_1D = []\n",
    "final_target_2D = []\n",
    "folders_2D = []\n",
    "data_dimensions = []\n",
    "for indx, spec in enumerate(data_dict.keys()):\n",
    "    workspace = dnplab.create_workspace(\"proc\", data_dict[spec])\n",
    "    dnplab.dnpNMR.remove_offset(workspace)\n",
    "    dnplab.dnpNMR.window(workspace, linewidth=10)\n",
    "    dnplab.dnpNMR.fourier_transform(workspace, zero_fill_factor=2)\n",
    "    dnplab.dnpNMR.autophase(workspace, force_positive=False)\n",
    "    data_dimensions.append(workspace[\"proc\"].ndim)\n",
    "    dnplab.dnpTools.integrate(workspace)\n",
    "    if data_dimensions[indx] == 1:\n",
    "        final_target_1D.append(workspace[\"proc\"].values)\n",
    "        folders_1D.append(spec)\n",
    "    elif data_dimensions[indx] == 2:\n",
    "        dnplab.dnpFit.exponential_fit(workspace, type=\"T1\")\n",
    "        final_target_2D.append(workspace[\"fit\"].attrs[\"T1\"])\n",
    "        folders_2D.append(spec)\n",
    "        \n",
    "\n",
    "folders_1D = list(map(int, folders_1D))\n",
    "folders_1D.sort()\n",
    "folders_2D = list(map(str, folders_2D))\n",
    "folders_2D.sort()\n",
    "\n",
    "powers_1D = dnplab.dnpIO.cnsi.get_powers(base_path, \"power\", folders_1D)\n",
    "powers_2D = dnplab.dnpIO.cnsi.get_powers(base_path, \"t1_powers\", folders_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now arranged into target format: list of amplitudes or \"T1\" each with corresponding power lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Modeling\"> </a>\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models exist for fitting data of this nature and have been programmed into a module within the DNPLab package. Some known parameters must be passed along with the data and physical constants are calculated using optimization routines. Pass the data to the correct module,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydration = {\n",
    "    \"E\": np.array(final_target_1D),\n",
    "    \"E_power\": np.array(powers_1D),\n",
    "    \"T1\": np.array(final_target_2D),\n",
    "    \"T1_power\": np.array(powers_2D),\n",
    "}\n",
    "hydration.update(\n",
    "    {\n",
    "        \"T10\": T10,\n",
    "        \"T100\": T100,\n",
    "        \"spin_C\": spin_C,\n",
    "        \"field\": field,\n",
    "        \"smax_model\": smax_model,\n",
    "        \"t1_interp_method\": t1_interp_method,\n",
    "    }\n",
    ")\n",
    "hyd = dnplab.create_workspace()\n",
    "hyd.add(\"hydration_inputs\", hydration)\n",
    "\n",
    "results = dnplab.dnpHydration.hydration(hyd)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Evaluation\"> </a>\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously published data were used to confirm the model produces the correct result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Deployment\"> </a>\n",
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example application was the batch processing of a large dataset for which each of the `base_folder`s had clues in their name but there was no available description of the dataset. I process the entire batch using the procedure above and try to correlate the clues in the folder names with the results. First lets make a function to perform the above procedure so that we can pass an entire batch, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(base_path):\n",
    " \n",
    "    # insert above code\n",
    "\n",
    "    return results[\"hydration_results\"][\"k_sigma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and loop over the entire set of base folders creating a dictionary of one aspect of the results that should inform on the character of the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_list = os.listdir(set_path)\n",
    "\n",
    "descriptive = []\n",
    "for indx, path in enumerate(base_list):\n",
    "    descriptive.append(process(path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets take apart the folder names and make a table of potential clues along with the descriptive for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder_list = pd.read_csv(\"../lst_dirs.csv\")\n",
    "\n",
    "data = []\n",
    "sample = []\n",
    "index = []\n",
    "for indx, name in enumerate(folder_list):\n",
    "    nm = name.split(\"_\")\n",
    "    date.append(nm[0])\n",
    "    sample.append(nm[2])\n",
    "    index.append(nm[3])\n",
    "\n",
    "\n",
    "descriptive_dict = {\"date\": date,\n",
    "                    \"sample\": sample,\n",
    "                    \"index\": index,\n",
    "                    \"descriptor\": descriptive,\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to evaluate correlations. For example, lets see how \"samp6\" correlates with the corresponding descriptors,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "sample = []\n",
    "time = []\n",
    "index = []\n",
    "for indx, name in enumerate(folder_list[folder_list.columns[0]]):\n",
    "    nm = name.split(\"_\")\n",
    "    if \"samp6\" in nm[2]:\n",
    "        date.append(nm[0])\n",
    "        sample.append(nm[2])\n",
    "        time.append(nm[3])\n",
    "        index.append(nm[4])\n",
    "\n",
    "\n",
    "descriptive_dict = {\"date\": date,\n",
    "                 \"sample\": sample,\n",
    "                 \"time\": time,\n",
    "                 \"index\": index,\n",
    "                 \"descriptor\": descriptive,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and lets make a plot of the descriptives,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b977ab33a7de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"descriptor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(descriptive_dict[\"descriptor\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the descriptors are in groups. Lets see if the groups correlate with \"time\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2e23208c0eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescriptive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"descriptor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(descriptive_dict[\"time\"],descriptive_dict[\"descriptor\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like they are tricplcate measurements of the same samples! The correlation also shows with index,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(descriptive_dict[\"index\"],descriptive_dict[\"descriptor\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
